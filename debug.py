# -*- coding: utf-8 -*-
"""L-layer NN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FMSDOyNMPmyRXZRQp8h-4c4bvAOp7xS1
"""

import numpy as np
import matplotlib.pyplot as plt

"""**utils**"""

def sigmoid(Z, dA = None):
    if dA == None:
        A = 1/(1+np.exp(-Z))
    else:
        A = dA *  (1/(1+np.exp(-Z))) * (1- 1/(1+np.exp(-Z)))
    return A

def tanh(Z, dA = None):
    if dA == None:
        A = np.tanh(Z)
    else:
        A = dA *  (1 - np.tanh(Z)**2)
    return A

def softmax(Z):
    num = np.exp(Z)
    denom = np.sum(num, axis = 0)
    A = num / denom
    return A
def relu(Z, dA = 0):
    if not isinstance(dA, np.ndarray):
        A = np.maximum(0, Z)
    else:
        dZ = np.array(dA, copy=True)
        dZ[Z <= 0] = 0
        A = dZ
    return A

def Cross_Entropy_Loss(O, Y,back = False):
    if not back:
        L = (-np.sum(np.log(O) * Y))/O.shape[0]
    else:
        L = O - Y
    return L

def initializer(input_dim, output_dim, method):
    if method == "Xavier":
        w_i = (6 / (input_dim + output_dim)) ** 0.5
        weights = np.random.uniform(-w_i, w_i, size = (input_dim,output_dim))
    
    elif method == "Normal":
        weights = np.random.normal(size = (output_dim, input_dim)).T
    
    elif method == "He":
        he = 2 / (input_dim) ** 0.5
        weights = np.random.rand(output_dim, input_dim) * he
    return weights

def mini_batch_generator(X, Y, batch_size):
    m = X.shape[0]  
    mini_batches = []

    idx = list(np.random.permutation(m))
    shuffled_X = X[idx]
    shuffled_Y = Y[idx]

    full_batches = math.floor(m/batch_size)
    for k in range(0, full_batches):
        mini_batch_X = shuffled_X[k * batch_size : (k+1) * batch_size]
        mini_batch_Y = shuffled_Y[ k * batch_size : (k+1) * batch_size]

        mini_batch = (mini_batch_X, mini_batch_Y)
        mini_batches.append(mini_batch)
    
    if m % batch_size != 0:
        mini_batch_X = shuffled_X[batch_size * math.floor(m / batch_size) : ]
        mini_batch_Y = shuffled_Y[batch_size * math.floor(m / batch_size) : ]

        mini_batch = (mini_batch_X, mini_batch_Y)
        mini_batches.append(mini_batch)
    
    return mini_batches

#from utils import softmax, sigmoid, tanh, relu, initializer

class Dense():
    def __init__(self, input_dim, output_dim, method = "Xavier", activation = "relu"):
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.activation = activation
        self.method = method
        self.weight = initializer(self.input_dim, self.output_dim, self.method)
        self.bais = np.zeros(( 1, output_dim))
    
    def update_params(self, w, b):
        self.weight = w.copy()
        self.bais = b.copy()
        return 

    def forward(self, X):
        # linear forward
        self.X = X
        #print(X.shape,'asdas')
        #print(self.weight.shape)
        self.Z = self.X @ self.weight  + self.bais

        # activation forward
        if self.activation == "softmax":
            A = softmax(self.Z)
        
        elif self.activation == "relu":
            A = relu(self.Z)
        
        elif self.activation == "sigmoid":
            A = sigmoid(self.Z)
        
        elif self.activation == "tanh":
            A = tanh(self.Z)

        return A

    def backward(self, dA):
        # derivative with respect to activation function        
        if self.activation == "relu":
            dZ = relu( self.Z,dA)
        
        elif self.activation == "sigmoid":
            dZ = sigmoid(dA, self.Z)
        
        elif self.activation == "tanh":
            dZ = tanh(dA, self.Z)
        else:
            dZ = dA
        m = self.X.shape[1]
        # derivative with respect to weights, bais and X(current state)
        self.dweight = 1/m * np.matmul(self.X.T, dZ )
        self.dbais = 1/m * np.sum(dZ, axis = 0, keepdims=True)
        self.dX = np.matmul( dZ, self.weight.T)
        
        return self.dX

import math
#from utils import mini_batch_generator

class NeuralNetwork():
    def __init__(self, layers, cost_function):
        self.layers = layers
        self.cost_function = cost_function
    
    def learn(self, X, Y, optimizer, max_epoch, batch_size):

        if self.cost_function == 'CrossEntopy':
            cost = Cross_Entropy_Loss

        else:
            print('invalid cost metric!')


        train_losses = []
        val_losses = []
        
        X = X
        Y = Y
        
        epoch = 0
        loss_list = []
        
        while epoch < max_epoch:
            loss = 0
            batches_list = mini_batch_generator(X, Y, batch_size)
            for batch in batches_list:
                A, label = batch
                for layer in self.layers:
                    A = layer.forward(A)
                
                loss += cost(A, label) / A.shape[0]

                dA = cost(A, label, back = True)
                for layer in self.layers[::-1]:
                    dA = layer.backward(dA)

                optimizer.step(self.layers)

            if epoch % 10 == 0:
                print ("Cost after epoch %i: %f" %(epoch, loss))
            loss_list.append(loss)
            epoch += 1
        return 

    def predict(self, data):
        A = data.copy()
        for layer in self.layers:
            A = layer.forward(A)
        
        return A

"""**OPTIMIZER**"""

class Optimizer():
    def __init__(self,method, learning_rate, beta, beta1, beta2, epsilon = 1e-8):
        self.method = method
        self.learning_rate = learning_rate
        self.t = 1
        self.beta = beta
        self.beta1 = beta1
        self.beta2 = beta2
        self.epsilon = epsilon
        self.flag = True

    def initializer(self, layer_list):
        if self.method == "gd":
            pass
        if self.method == "sgd":
            pass
        if self.method == "momentum":
            pass
        if self.method == "adam":
            self.v = {}
            self.s = {}
            w = []
            b = []
            for layer in layer_list: 
                w.append(np.zeros(layer.weight.shape))
                b.append(np.zeros(layer.bais.shape))
            self.v['W'] = w.copy()
            self.v['b'] = b.copy()
            self.s['W'] = w.copy()
            self.s['b'] = b.copy()


    def step(self, layer_list):
        if self.method == "gd":
            pass
        if self.method == "sgd":
            pass
        if self.method == "momentum":
            pass
        if self.method == "adam":
            if self.flag:
                 self.initializer(layer_list)
                 self.flag = False
                 
            v_corrected = {'W':[],  'b':[]}                      
            s_corrected = {'W':[],  'b':[]} 
            for i, layer in enumerate(layer_list):
                self.v['W'][i] = self.beta1 * self.v['W'][i] + (1-self.beta1) * layer.dweight 
                self.v['b'][i] = self.beta1 * self.v['b'][i] + (1-self.beta1) * layer.dbais 
            
                v_corrected['W'].append((self.v["W"][i] / (1-self.beta1**self.t)))
                v_corrected['b'].append((self.v["b"][i] / (1-self.beta1**self.t)))
            
                self.s['W'][i] = self.beta2 * self.s['W'][i] + (1-self.beta2) * layer.dweight **2
                self.s['b'][i] = self.beta2 * self.s['b'][i] + (1-self.beta2) * layer.dbais **2
            
                s_corrected['W'].append((self.s["W"][i] / (1-self.beta2**self.t)))
                s_corrected['b'].append((self.s["b"][i] / (1-self.beta2**self.t)))
            
                new_W = layer.weight -(self.learning_rate * v_corrected['W'][i] / (s_corrected["W"][i]**(0.5) + self.epsilon))
                new_b = layer.bais -(self.learning_rate * v_corrected['b'][i] / (s_corrected["b"][i]**(0.5) + self.epsilon))
                layer.update_params(new_W,new_b)
        
        self.t += 1
        return

class Encoder():
    def __init__(self,one_hot = True):
        self.one_hot = one_hot
    
    def learn(self,X):
        self.label_map_list = {}
        self.num_features = X.shape[1]
        self.one_hot_list = {}
        self.categorical_idx = []

        X_copy = X.copy()
        for col_idx in range(self.num_features):
            col = X_copy[:,col_idx]
           
            
            if not np.issubdtype(type(col[0]), np.number):
                #print(type(col[0]))
                foo_map = {}
                self.categorical_idx.append(col_idx)

                u, indices = np.unique(col, return_index=True)

                for i, uniq in enumerate(u):
                    col[col == uniq] = i
                    foo_map[uniq] = i

                col = col.astype(int)
                if self.one_hot:
                    hot = np.zeros((col.size, col.max() + 1))
                    hot[np.arange(col.size), col] = 1
                else:
                    hot = col.reshape((-1,1))

                self.one_hot_list[str(col_idx)] = hot
                self.label_map_list[str(col_idx)] = foo_map
        return self

    def execute(self,X):

        for i in self.one_hot_list.values():
            X = np.concatenate((X, i),axis = 1)
            #print(X)

        X = np.delete(X,self.categorical_idx,1)
            
        return X.astype(float).astype(int)

    def fast(self,X):
        return self.learn(X).execute(X)

import pandas as pd

class OutlierRemoval():
    def __init__(self, X, Y, threshold):
        self.X = X
        self.Y = Y
        self.threshold = threshold
    
    def learn(self, sdv_norm = False):

        if not sdv_norm:
            self.X = (self.X - np.mean(self.X, axis = 0)) / np.std(self.X, axis = 0)

        return self

    def execute(self):
        data = np.hstack((self.X, self.Y))

        clean_X = data[np.all(abs(self.X) < self.threshold, axis=1)]
        self.outlier = data[~np.all(abs(self.X) < self.threshold, axis=1)]
        
        return clean_X
    
    def fast(self, sdv_norm = False):
        return self.learn(sdv_norm).execute()

data = pd.read_csv("/workspaces/ML/raw_data/archive/sdss-IV-dr16-70k.csv")

data = data.iloc[:, 1: 13]
data = data.sort_values(by = ["class"])
data = data.drop(columns=["run", "rerun", "camcol"])

# g_data = data[:30]
# s_data = data[-30:]
# q_data = data[55000: 55030]

# train_data = pd.concat([q_data, s_data, g_data])


X = data.iloc[:, :-1].to_numpy()
Y = data.iloc[:, -1].to_numpy().reshape(-1,1)

out_remove = OutlierRemoval(X, Y, 3)
clean_data = out_remove.fast(False)

X = clean_data[:, :-1].astype(float)
Y = clean_data[:, -1]

enc = Encoder()
Y = enc.fast(Y.reshape(-1,1))

layers = [Dense(8, 32), Dense(32, 8), Dense(8, 3, activation = "softmax")]
model_NN = NeuralNetwork(layers, "CrossEntopy")
adam = Optimizer("adam", 0.01, None, 0.9, 0.99)

model_NN.learn(X, Y, optimizer = adam, max_epoch = 100, batch_size = 64)

